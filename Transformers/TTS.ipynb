{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lg2qIpWDud5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, QuantoConfig, GenerationConfig\n",
        "\n",
        "# load hf config\n",
        "hf_config = AutoConfig.from_pretrained(\"MiniMaxAI/MiniMax-Text-01\", trust_remote_code=True)\n",
        "\n",
        "# quantization config, int8 is recommended\n",
        "quantization_config =  QuantoConfig(\n",
        "            weights=\"int8\",\n",
        "            modules_to_not_convert=[\n",
        "                \"lm_head\",\n",
        "                \"embed_tokens\",\n",
        "            ] + [f\"model.layers.{i}.coefficient\" for i in range(hf_config.num_hidden_layers)]\n",
        "            + [f\"model.layers.{i}.block_sparse_moe.gate\" for i in range(hf_config.num_hidden_layers)]\n",
        "        )\n",
        "\n",
        "# assume 8 GPUs\n",
        "world_size = 8\n",
        "layers_per_device = hf_config.num_hidden_layers // world_size\n",
        "# set device map\n",
        "device_map = {\n",
        "    'model.embed_tokens': 'cuda:0',\n",
        "    'model.norm': f'cuda:{world_size - 1}',\n",
        "    'lm_head': f'cuda:{world_size - 1}'\n",
        "}\n",
        "for i in range(world_size):\n",
        "    for j in range(layers_per_device):\n",
        "        device_map[f'model.layers.{i * layers_per_device + j}'] = f'cuda:{i}'\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"MiniMaxAI/MiniMax-Text-01\")\n",
        "prompt = \"Hello!\"\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant created by MiniMax based on MiniMax-Text-01 model.\"}]},\n",
        "    {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]},\n",
        "]\n",
        "text = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "# tokenize and move to device\n",
        "model_inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# load bfloat16 model, move to device, and apply quantization\n",
        "quantized_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"MiniMaxAI/MiniMax-Text-01\",\n",
        "    torch_dtype=\"bfloat16\",\n",
        "    device_map=device_map,\n",
        "    quantization_config=quantization_config,\n",
        "    trust_remote_code=True,\n",
        "    offload_buffers=True,\n",
        ")\n",
        "\n",
        "# generate response\n",
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=20,\n",
        "    eos_token_id=200020,\n",
        "    use_cache=True,\n",
        ")\n",
        "generated_ids = quantized_model.generate(**model_inputs, generation_config=generation_config)\n",
        "print(f\"generated_ids: {generated_ids}\")\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n"
      ]
    }
  ]
}