{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mrcnn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRgbyaQfEcLz"
      },
      "source": [
        "!git clone https://github.com/deepanrajm/deep_learning.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wft834yYzATS"
      },
      "source": [
        "! pip install mrcnn==0.1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ml59UuOAB1ci"
      },
      "source": [
        "! pip uninstall opencv-python tensorflow keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcNcQAsbB7YY"
      },
      "source": [
        "!pip install opencv-python==3.4.0.12 tensorflow==1.15 keras==2.0.8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnG-tMfDiLWm"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import mrcnn.config\n",
        "import mrcnn.visualize\n",
        "import mrcnn.utils\n",
        "from mrcnn.model import MaskRCNN\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def visualize_detections(image, masks, boxes, class_ids):\n",
        "    # Create a new solid-black image the same size as the original image\n",
        "    masked_image = np.zeros(image.shape)\n",
        "\n",
        "    # Loop over each detected object's mask\n",
        "    for i in range(masks.shape[2]):\n",
        "        # If the detected object isn't a person (class_id == 1), skip it\n",
        "        if class_ids[i] != 1:\n",
        "            continue\n",
        "\n",
        "        # Draw the mask for the current object in white\n",
        "        mask = masks[:, :, i]\n",
        "        color = (1.0, 1.0, 1.0) # White\n",
        "        masked_image = mrcnn.visualize.apply_mask(masked_image, mask, color, alpha=1.0)\n",
        "\n",
        "    # Use Morphological operations (dilate and erode) to find large blobs of people\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    masked_image = cv2.morphologyEx(masked_image, cv2.MORPH_DILATE, kernel, iterations=25)\n",
        "\n",
        "    # Convert the masked image to pure black and white (1-bit)\n",
        "    image_bw = cv2.cvtColor(masked_image.astype(np.uint8), cv2.COLOR_BGR2GRAY)\n",
        "    thresh, image_bw = cv2.threshold(image_bw, 220, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    # Find the single largest contour area which\n",
        "    (_, contours, _) = cv2.findContours(image_bw, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours = sorted(contours, key=cv2.contourArea, reverse=True)[:1]\n",
        "\n",
        "    # Find the bounding box of the largest contour\n",
        "    bounding_x1, bounding_y1, bounding_w, bounding_h = cv2.boundingRect(contours[0])\n",
        "\n",
        "    # Get the coords of the bottom right corner of the bounding box\n",
        "    bounding_x2 = bounding_x1 + bounding_w\n",
        "    bounding_y2 = bounding_y1 + bounding_h\n",
        "\n",
        "    bgr_image = image[:, :, ::-1]\n",
        "    font = cv2.FONT_HERSHEY_DUPLEX\n",
        "\n",
        "    person_count = 0\n",
        "\n",
        "    # Loop over each detected person\n",
        "    for i in range(boxes.shape[0]):\n",
        "        # If the detected object isn't a person (class_id == 1), skip it\n",
        "        if class_ids[i] != 1:\n",
        "            continue\n",
        "\n",
        "        # Get the bounding box of the current person\n",
        "        y1, x1, y2, x2 = boxes[i]\n",
        "\n",
        "        # Check if this person is inside the overall line's bounding box\n",
        "        if x1 >= bounding_x1 and x2 <= bounding_x2 and y1 >= bounding_y1 and y2 <= bounding_y2:\n",
        "            person_count += 1\n",
        "\n",
        "           # Draw a mask for the current person\n",
        "            mask = masks[:, :, i]\n",
        "            color = (1.0, 1.0, 1.0) # White\n",
        "            image = mrcnn.visualize.apply_mask(image, mask, color, alpha=0.6)\n",
        "\n",
        "    # Draw a box around the whole line area\n",
        "    cv2.rectangle(bgr_image, (bounding_x1, bounding_y1), (bounding_x2, bounding_y2), (0, 0, 255), 14)\n",
        "\n",
        "    # Label the number of people in line\n",
        "    cv2.putText(bgr_image, f\"{person_count} in line\", (bounding_x1, bounding_y1-20), font, 2.0, (0, 0, 255), 5)\n",
        "\n",
        "    # Convert the image back to RGB\n",
        "    rgb_image = bgr_image[:, :, ::-1]\n",
        "\n",
        "    # Return the image and the number of people in line\n",
        "    return person_count, rgb_image.astype(np.uint8)\n",
        "\n",
        "\n",
        "# Configuration that will be used by the Mask-RCNN library\n",
        "class MaskRCNNConfig(mrcnn.config.Config):\n",
        "    NAME = \"coco_pretrained_model_config\"\n",
        "    IMAGES_PER_GPU = 1\n",
        "    GPU_COUNT = 1\n",
        "    NUM_CLASSES = 1 + 80  # COCO dataset has 80 classes + one background class\n",
        "\n",
        "\n",
        "# Root directory of the project\n",
        "#ROOT_DIR = Path(\".\")\n",
        "\n",
        "# Directory to save logs and trained model\n",
        "#MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "MODEL_DIR = \"deep_learning/MRCNN/logs\"\n",
        "\n",
        "# Local path to trained weights file\n",
        "#COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "COCO_MODEL_PATH = \"mask_rcnn_coco.h5\"\n",
        "\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory of images to run detection on\n",
        "#IMAGE_DIR = os.path.join(ROOT_DIR, \"images\")\n",
        "\n",
        "# Create a Mask-RCNN model in inference mode\n",
        "model = MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=MaskRCNNConfig())\n",
        "\n",
        "# Load pre-trained model\n",
        "model.load_weights(COCO_MODEL_PATH, by_name=True)\n",
        "\n",
        "# Load the image we want to run detection on\n",
        "#image_path = str(ROOT_DIR / \"sample_images\" / \"line_example.png\")\n",
        "image_path = \"deep_learning/MRCNN/line_example.png\"\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Convert the image from BGR color (which OpenCV uses) to RGB color\n",
        "rgb_image = image[:, :, ::-1]\n",
        "\n",
        "# Run the image through the model\n",
        "results = model.detect([rgb_image], verbose=1)\n",
        "\n",
        "# Visualize results\n",
        "r = results[0]\n",
        "people_in_line_count, masked_image = visualize_detections(rgb_image, r['masks'], r['rois'], r['class_ids'])\n",
        "print(f\"Found {people_in_line_count} people in line outside the building\")\n",
        "\n",
        "# Show the result on the screen\n",
        "plt.imshow(masked_image.astype(np.uint8))\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}