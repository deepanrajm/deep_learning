{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffbfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51dd7bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = 'sales_report.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a938f553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Setup the LLM ---\n",
    "# Point this to your local LLM server (e.g., LM Studio, Ollama)\n",
    "llm = ChatOpenAI(openai_api_base=\"http://localhost:1234/v1\", openai_api_key=\"lm_studio\", model=\"local-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc431293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load and Process the CSV Data ---\n",
    "print(\"\\nLoading CSV data...\")\n",
    "loader = CSVLoader(file_path=csv_file_path)\n",
    "docs = loader.load()\n",
    "\n",
    "# Display content of the first loaded document for verification\n",
    "# print(\"\\nContent of the first document chunk:\")\n",
    "# print(docs[0].page_content)\n",
    "\n",
    "print(\"\\nSplitting documents into semantic chunks...\")\n",
    "# Using a local embedding model for chunking\n",
    "text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(f\"Created {len(documents)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Create Vector Store ---\n",
    "print(\"\\nCreating vector store with embeddings...\")\n",
    "# Instantiate the embedding model\n",
    "embedder = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector = FAISS.from_documents(documents, embedder)\n",
    "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "print(\"Vector store created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b50da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Define the RAG Chain ---\n",
    "print(\"\\nSetting up the RAG chain...\")\n",
    "# This prompt template is designed to guide the LLM to act as a sales analyst.\n",
    "prompt_template = \"\"\"\n",
    "1. You are a helpful sales analyst.\n",
    "2. Use the following pieces of context from the sales report to answer the question at the end.\n",
    "3. If you don't know the answer from the context, just say that you don't know. Don't try to make up an answer.\n",
    "4. Provide a concise answer based only on the provided sales data.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# LLM Chain\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=QA_CHAIN_PROMPT,\n",
    "    verbose=False # Set to True for more detailed logs\n",
    ")\n",
    "\n",
    "# Document processing chain\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    document_variable_name=\"context\",\n",
    "    document_prompt=PromptTemplate(\n",
    "        input_variables=[\"page_content\"],\n",
    "        template=\"Row:\\n{page_content}\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "# The final RetrievalQA chain\n",
    "qa_chain = RetrievalQA(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    verbose=False, \n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "print(\"RAG chain is ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72df6d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 6. Query the Sales Data ---\n",
    "\n",
    "\n",
    "query1 = \"What was the total sale amount for the North region?\"\n",
    "print(f\"\\nQuestion: {query1}\")\n",
    "result1 = qa_chain({\"query\": query1})\n",
    "print(\"Answer:\", result1[\"result\"].strip())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2f99a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"how many laptops sold in south region\"\n",
    "print(f\"\\nQuestion: {query2}\")\n",
    "result2 = qa_chain({\"query\": query2})\n",
    "print(\"Answer:\", result2[\"result\"].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
