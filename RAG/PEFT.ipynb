{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 1: Install Dependencies\n",
        "# ========================\n",
        "\n",
        "!pip install transformers datasets peft accelerate bitsandbytes evaluate rouge_score trl -q"
      ],
      "metadata": {
        "id": "OMlC6f8XBySy"
      },
      "id": "OMlC6f8XBySy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 2: Load Dataset and Tokenizer\n",
        "# ========================\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "dataset = load_dataset(\"knkarthick/samsum\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"
      ],
      "metadata": {
        "id": "Gj4PXcxQBz2R"
      },
      "id": "Gj4PXcxQBz2R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 3: Evaluate Base Model\n",
        "# ========================\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "import evaluate\n",
        "import torch\n",
        "\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\").to(\"cuda\")\n",
        "base_model.eval()\n",
        "\n",
        "def evaluate_model(model, dataset, tokenizer, n=20):\n",
        "    preds, refs = [], []\n",
        "    for example in dataset.select(range(n)):\n",
        "        prompt = f\"Summarize this dialogue:\\n\\n{example['dialogue']}\\n\\nSummary:\"\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        with torch.no_grad():\n",
        "            output = model.generate(**inputs, max_new_tokens=60)\n",
        "        summary = tokenizer.decode(output[0], skip_special_tokens=True).split(\"Summary:\")[-1]\n",
        "        preds.append(summary.strip())\n",
        "        refs.append(example[\"summary\"])\n",
        "    return rouge.compute(predictions=preds, references=refs)\n",
        "\n",
        "base_score = evaluate_model(base_model, dataset[\"test\"], tokenizer)\n",
        "print(\"ROUGE (Base Model):\", base_score)"
      ],
      "metadata": {
        "id": "oblay37XB1k1"
      },
      "id": "oblay37XB1k1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 4: Preprocess Function\n",
        "# ========================\n",
        "\n",
        "def preprocess(examples):\n",
        "    prompts = [f\"Summarize this dialogue:\\n\\n{d}\\n\\nSummary:\" for d in examples[\"dialogue\"]]\n",
        "    inputs = tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=512)\n",
        "    targets = tokenizer(examples[\"summary\"], truncation=True, padding=\"max_length\", max_length=60)\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "train_data = dataset[\"train\"].map(preprocess, batched=True)"
      ],
      "metadata": {
        "id": "1Uy08ReMB4JC"
      },
      "id": "1Uy08ReMB4JC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 5: LoRA Fine-tuning\n",
        "# ========================\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "lora_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\").to(\"cuda\")\n",
        "lora_model.gradient_checkpointing_enable()\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(lora_model, lora_config)\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora-out\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=500,\n",
        "    logging_steps=100,\n",
        "    fp16=True,\n",
        "    save_total_limit=1,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=lora_model, args=training_args, train_dataset=train_data, data_collator=collator)\n",
        "trainer.train()\n",
        "\n",
        "lora_score = evaluate_model(lora_model, dataset[\"test\"], tokenizer)\n",
        "print(\"ROUGE (LoRA):\", lora_score)"
      ],
      "metadata": {
        "id": "ZCIgYKAoB6WE"
      },
      "id": "ZCIgYKAoB6WE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 6: QLoRA Fine-tuning\n",
        "# ========================\n",
        "\n",
        "from transformers import BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "qlora_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", quantization_config=bnb_config, device_map=\"auto\")\n",
        "qlora_model = prepare_model_for_kbit_training(qlora_model)\n",
        "qlora_model = get_peft_model(qlora_model, lora_config)\n",
        "\n",
        "trainer = Trainer(model=qlora_model, args=training_args, train_dataset=train_data, data_collator=collator)\n",
        "trainer.train()\n",
        "\n",
        "qlora_score = evaluate_model(qlora_model, dataset[\"test\"], tokenizer)\n",
        "print(\"ROUGE (QLoRA):\", qlora_score)"
      ],
      "metadata": {
        "id": "U73lH5GmB76h"
      },
      "id": "U73lH5GmB76h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 7: Soft Prompting (Prefix-Tuning)\n",
        "# ========================\n",
        "\n",
        "from peft import PromptTuningConfig\n",
        "\n",
        "prompt_config = PromptTuningConfig(\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    prompt_tuning_init=\"TEXT\",\n",
        "    num_virtual_tokens=20,\n",
        "    tokenizer_name_or_path=tokenizer.name_or_path,\n",
        "    prompt_tuning_init_text=\"Summarize the following conversation:\"\n",
        ")\n",
        "\n",
        "soft_prompt_model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\").to(\"cuda\")\n",
        "soft_prompt_model = get_peft_model(soft_prompt_model, prompt_config)\n",
        "\n",
        "trainer = Trainer(model=soft_prompt_model, args=training_args, train_dataset=train_data, data_collator=collator)\n",
        "trainer.train()\n",
        "\n",
        "prompt_score = evaluate_model(soft_prompt_model, dataset[\"test\"], tokenizer)\n",
        "print(\"ROUGE (Soft Prompt):\", prompt_score)"
      ],
      "metadata": {
        "id": "o6dX-kXqB9jl"
      },
      "id": "o6dX-kXqB9jl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================\n",
        "# STEP 8: Summary Table\n",
        "# ========================\n",
        "print(\"========= ROUGE Summary =========\")\n",
        "print(f\"Base Model:      {base_score}\")\n",
        "print(f\"LoRA Model:      {lora_score}\")\n",
        "print(f\"QLoRA Model:     {qlora_score}\")\n",
        "print(f\"Soft Prompting:  {prompt_score}\")\n"
      ],
      "metadata": {
        "id": "n1Ycx0vGBOpy"
      },
      "id": "n1Ycx0vGBOpy",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}