{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the PDF: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Analysis of the Example:\\n• Modifier - Blog Post: Here, we are instructing the Language Model to create a\\nmore detailed and in-depth text, as opposed to short, concise tweets.\\n• Topic - Healthy Eating: This is the main focus of our request.\\n• Target Audience - Working Professionals: The model needs to present the\\ninformation in a way that is practical and actionable for working professionals\\nwho may have limited time.\\n• Use of Keywords - SEO Relevance: The model should incorporate keywords\\nand phrases that can enhance visibility on search engines.\\n• Style - Simple, Understandable Style: The model should make the text clear\\nand comprehensible to facilitate easy reading.\\n• Length and Structure - 800 Words, Well-Structured: These requirements help\\ndetermine the length and formatting of the text.\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = PDFPlumberLoader(r\"D:\\Petramount\\Courses\\AI\\LLM\\1+The+structured+Prompt.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Check the number of pages\n",
    "print(\"Number of pages in the PDF:\",len(docs))\n",
    "\n",
    "# Load the random page content\n",
    "docs[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\.conda\\envs\\rag\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "text_splitter = SemanticChunker(HuggingFaceEmbeddings())\n",
    "documents = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created:  4\n",
      "Analysis of the Example:\n",
      "• Modifier - Blog Post: Here, we are instructing the Language Model to create a\n",
      "more detailed and in-depth text, as opposed to short, concise tweets. • Topic - Healthy Eating: This is the main focus of our request.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of chunks created: \", len(documents))\n",
    "\n",
    "print(documents[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the embedding model\n",
    "embedder = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store \n",
    "vector = FAISS.from_documents(documents, embedder)\n",
    "retriever = vector.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 20\u001b[0m\n\u001b[0;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m1. Use the following pieces of context to answer the question at the end.\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m2. If you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, just say that \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt make up an answer on your own.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[38;5;124mHelpful Answer:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     18\u001b[0m QA_CHAIN_PROMPT \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(prompt) \n\u001b[1;32m---> 20\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m \u001b[43mLLMChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQA_CHAIN_PROMPT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m document_prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[0;32m     27\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage_content\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     28\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mcontent:\u001b[39m\u001b[38;5;132;01m{page_content}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msource:\u001b[39m\u001b[38;5;132;01m{source}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m StuffDocumentsChain(\n\u001b[0;32m     32\u001b[0m                   llm_chain\u001b[38;5;241m=\u001b[39mllm_chain,\n\u001b[0;32m     33\u001b[0m                   document_variable_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m                   document_prompt\u001b[38;5;241m=\u001b[39mdocument_prompt,\n\u001b[0;32m     35\u001b[0m                   callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     36\u001b[0m               )\n",
      "File \u001b[1;32mc:\\Users\\deepa\\.conda\\envs\\rag\\lib\\site-packages\\pydantic\\v1\\main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[0;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[1;31mValidationError\u001b[0m: 2 validation errors for LLMChain\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)\nllm\n  instance of Runnable expected (type=type_error.arbitrary_type; expected_arbitrary_type=Runnable)"
     ]
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = \"\"\"\n",
    "1. Use the following pieces of context to answer the question at the end.\n",
    "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\\n\n",
    "3. Keep the answer crisp and limited to 3,4 sentences.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(prompt) \n",
    "\n",
    "llm_chain = LLMChain(\n",
    "                  llm=llm, \n",
    "                  prompt=QA_CHAIN_PROMPT, \n",
    "                  callbacks=None, \n",
    "                  verbose=True)\n",
    "\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\", \"source\"],\n",
    "    template=\"Context:\\ncontent:{page_content}\\nsource:{source}\",\n",
    ")\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "                  llm_chain=llm_chain,\n",
    "                  document_variable_name=\"context\",\n",
    "                  document_prompt=document_prompt,\n",
    "                  callbacks=None,\n",
    "              )\n",
    "\n",
    "qa = RetrievalQA(\n",
    "                  combine_documents_chain=combine_documents_chain,\n",
    "                  verbose=True,\n",
    "                  retriever=retriever,\n",
    "                  return_source_documents=True,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\deepa\\.conda\\envs\\rag\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "1. Use the following pieces of context to answer the question at the end.\n",
      "2. If you don't know the answer, just say that \"I don't know\" but don't make up an answer on your own.\n",
      "\n",
      "3. Keep the answer crisp and limited to 3,4 sentences.\n",
      "\n",
      "Context: Context:\n",
      "content:Structured Prompt! A structured prompt consists of a (Modifier) + (Topic) + (Multiple Modifiers). These elements together form the optimized prompt. Modifier: Specifies the type of desired response. Examples: \"Twitter thread,\"\n",
      "\"blog post,\" \"research paper,\" etc. Topic: The main subject or question to be addressed in the response. Additional Modifiers: Additional specific requirements or details to be\n",
      "considered in the response. Examples: Target audience, keywords used, style,\n",
      "length, structure, etc.\n",
      "source:D:\\Petramount\\Courses\\AI\\LLM\\1+The+structured+Prompt.pdf\n",
      "\n",
      "Context:\n",
      "content:Analysis of the Example:\n",
      "• Modifier - Blog Post: Here, we are instructing the Language Model to create a\n",
      "more detailed and in-depth text, as opposed to short, concise tweets. • Topic - Healthy Eating: This is the main focus of our request.\n",
      "source:D:\\Petramount\\Courses\\AI\\LLM\\1+The+structured+Prompt.pdf\n",
      "\n",
      "Context:\n",
      "content:• Target Audience - Working Professionals: The model needs to present the\n",
      "information in a way that is practical and actionable for working professionals\n",
      "who may have limited time. • Use of Keywords - SEO Relevance: The model should incorporate keywords\n",
      "and phrases that can enhance visibility on search engines. • Style - Simple, Understandable Style: The model should make the text clear\n",
      "and comprehensible to facilitate easy reading. • Length and Structure - 800 Words, Well-Structured: These requirements help\n",
      "determine the length and formatting of the text. \n",
      "source:D:\\Petramount\\Courses\\AI\\LLM\\1+The+structured+Prompt.pdf\n",
      "\n",
      "Question: prompting example\n",
      "\n",
      "Helpful Answer:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "I don't know how to provide a prompt that meets these specifications, as it requires specifying a particular type of desired response (e.g. Twitter thread) and additional requirements such as keywords, style, length, and structure.\n",
      "\n",
      "A correct prompt would be something like:\n",
      "\n",
      "\"Structure a 800-word blog post on healthy eating for working professionals, incorporating SEO relevance, simple understanding, and well-structured format.\"\n",
      "\n",
      "I don't know how to provide an example that meets these specifications.\n"
     ]
    }
   ],
   "source": [
    "print(qa(\"prompting example\")[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"not-needed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
